{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e4bcdd",
   "metadata": {},
   "source": [
    "## What is RAG ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27668802",
   "metadata": {},
   "source": [
    "RAG is a technique where a language model first looks up (retrieves) useful information from a database or documents, and then uses that information to give a better answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee7c36",
   "metadata": {},
   "source": [
    "## Why and when we prefer RAG over finetuning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632852af",
   "metadata": {},
   "source": [
    "We **prefer RAG over finetuning** when: We want the model to give **up-to-date or specific answers** from **our own data** without changing the model itself.\n",
    "\n",
    "---\n",
    "\n",
    "**Why prefer RAG?**\n",
    "\n",
    "* **Cheaper & faster** – No need to train the model again.\n",
    "* **Easier to update** – Just change the documents, not the model.\n",
    "* **Better for private or large data** – You keep data separate and safe.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use RAG?**\n",
    "\n",
    "* When your data **changes often** (like news, product lists).\n",
    "* When you want the model to **answer from your documents**.\n",
    "* When **training a model is too costly or slow**.\n",
    "\n",
    "---\n",
    "\n",
    "Think of RAG like **giving the model a library to read** instead of teaching it everything from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac356886",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0436fd6",
   "metadata": {},
   "source": [
    "* **`langchain`** – Core framework to build LLM-powered applications.\n",
    "* **`langchain-community`** – Extra integrations like tools, APIs, and vector stores.\n",
    "* **`langchain-pinecone`** – Connects LangChain with Pinecone for vector storage and retrieval.\n",
    "* **`langchain_groq`** – Enables LangChain to use Groq's ultra-fast language models.\n",
    "* **`datasets`** – Provides ready-to-use NLP/ML datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65db921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.23\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community==0.3.21\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-pinecone==0.2.5\n",
      "  Downloading langchain_pinecone-0.2.5-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain_groq in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.6)\n",
      "Collecting datasets==3.5.0\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (0.3.9)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain==0.3.23)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==0.3.23) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (3.12.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.3.21) (2.2.6)\n",
      "Collecting pinecone<7.0.0,>=6.0.0 (from pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5)\n",
      "  Downloading pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.21)\n",
      "  Downloading aiohttp-3.10.11-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone==0.2.5)\n",
      "  Downloading langchain_tests-0.3.22-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (21.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.5.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.5.0)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.5.0)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets==3.5.0)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets==3.5.0) (25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (6.6.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (3.0.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain==0.3.23)\n",
      "  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pytest<9.0.0,>=7.0.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (8.4.2)\n",
      "Requirement already satisfied: pytest-asyncio<2.0.0,>=0.20.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.2.0)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting syrupy<5.0.0,>=4.0.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pytest-socket<1.0.0,>=0.7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_codspeed-4.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting vcrpy<8.0.0,>=7.0.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (0.23.0)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.19.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain==0.3.23) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.23) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (1.1.0)\n",
      "Collecting wrapt (from vcrpy<8.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: propcache>=0.2.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (0.3.2)\n",
      "Requirement already satisfied: groq<1,>=0.29.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_groq) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets==3.5.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets==3.5.0) (2025.2)\n",
      "Collecting py-cpuinfo (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: cffi>=1.17.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.0.0)\n",
      "Collecting rich>=13.8.1 (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 11.4 MB/s  0:00:00\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 26.5 MB/s  0:00:00\n",
      "Downloading langchain_pinecone-0.2.5-py3-none-any.whl (16 kB)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading aiohttp-3.10.11-cp313-cp313-win_amd64.whl (378 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading langchain_tests-0.3.22-py3-none-any.whl (49 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pinecone-6.0.2-py3-none-any.whl (421 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
      "Downloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 564.3/564.3 kB 15.3 MB/s  0:00:00\n",
      "Downloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pytest_codspeed-4.1.1-py3-none-any.whl (113 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
      "Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Installing collected packages: py-cpuinfo, wrapt, pinecone-plugin-interface, mdurl, fsspec, dill, vcrpy, syrupy, pytest-socket, pytest-benchmark, pinecone, multiprocess, markdown-it-py, huggingface-hub, httpx, aiohttp, rich, pytest-recording, langsmith, pytest-codspeed, langchain-core, datasets, langchain-tests, langchain-pinecone, langchain, langchain-community\n",
      "\n",
      "   ----------------------------------------  0/26 [py-cpuinfo]\n",
      "   --- ------------------------------------  2/26 [pinecone-plugin-interface]\n",
      "   ---- -----------------------------------  3/26 [mdurl]\n",
      "  Attempting uninstall: fsspec\n",
      "   ---- -----------------------------------  3/26 [mdurl]\n",
      "    Found existing installation: fsspec 2025.9.0\n",
      "   ---- -----------------------------------  3/26 [mdurl]\n",
      "    Uninstalling fsspec-2025.9.0:\n",
      "   ---- -----------------------------------  3/26 [mdurl]\n",
      "      Successfully uninstalled fsspec-2025.9.0\n",
      "   ---- -----------------------------------  3/26 [mdurl]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------ ---------------------------------  4/26 [fsspec]\n",
      "   ------- --------------------------------  5/26 [dill]\n",
      "   ------- --------------------------------  5/26 [dill]\n",
      "   ------- --------------------------------  5/26 [dill]\n",
      "   ------- --------------------------------  5/26 [dill]\n",
      "   --------- ------------------------------  6/26 [vcrpy]\n",
      "   --------- ------------------------------  6/26 [vcrpy]\n",
      "   ---------- -----------------------------  7/26 [syrupy]\n",
      "   ---------- -----------------------------  7/26 [syrupy]\n",
      "   ------------ ---------------------------  8/26 [pytest-socket]\n",
      "   ------------- --------------------------  9/26 [pytest-benchmark]\n",
      "   ------------- --------------------------  9/26 [pytest-benchmark]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   --------------- ------------------------ 10/26 [pinecone]\n",
      "   ---------------- ----------------------- 11/26 [multiprocess]\n",
      "   ---------------- ----------------------- 11/26 [multiprocess]\n",
      "   ---------------- ----------------------- 11/26 [multiprocess]\n",
      "   ---------------- ----------------------- 11/26 [multiprocess]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   ------------------ --------------------- 12/26 [markdown-it-py]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "  Attempting uninstall: httpx\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "    Found existing installation: httpx 0.27.2\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "    Uninstalling httpx-0.27.2:\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "   -------------------- ------------------- 13/26 [huggingface-hub]\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "  Attempting uninstall: aiohttp\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "    Found existing installation: aiohttp 3.12.14\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "    Uninstalling aiohttp-3.12.14:\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "      Successfully uninstalled aiohttp-3.12.14\n",
      "   --------------------- ------------------ 14/26 [httpx]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ----------------------- ---------------- 15/26 [aiohttp]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "  Attempting uninstall: langsmith\n",
      "   ------------------------ --------------- 16/26 [rich]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "    Found existing installation: langsmith 0.4.8\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "    Uninstalling langsmith-0.4.8:\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "      Successfully uninstalled langsmith-0.4.8\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   --------------------------- ------------ 18/26 [langsmith]\n",
      "   ----------------------------- ---------- 19/26 [pytest-codspeed]\n",
      "  Attempting uninstall: langchain-core\n",
      "   ----------------------------- ---------- 19/26 [pytest-codspeed]\n",
      "    Found existing installation: langchain-core 0.3.72\n",
      "   ----------------------------- ---------- 19/26 [pytest-codspeed]\n",
      "    Uninstalling langchain-core-0.3.72:\n",
      "   ----------------------------- ---------- 19/26 [pytest-codspeed]\n",
      "      Successfully uninstalled langchain-core-0.3.72\n",
      "   ----------------------------- ---------- 19/26 [pytest-codspeed]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   ------------------------------ --------- 20/26 [langchain-core]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   -------------------------------- ------- 21/26 [datasets]\n",
      "   --------------------------------- ------ 22/26 [langchain-tests]\n",
      "   --------------------------------- ------ 22/26 [langchain-tests]\n",
      "   ----------------------------------- ---- 23/26 [langchain-pinecone]\n",
      "  Attempting uninstall: langchain\n",
      "   ----------------------------------- ---- 23/26 [langchain-pinecone]\n",
      "    Found existing installation: langchain 0.3.27\n",
      "   ----------------------------------- ---- 23/26 [langchain-pinecone]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "    Uninstalling langchain-0.3.27:\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "      Successfully uninstalled langchain-0.3.27\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "  Attempting uninstall: langchain-community\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "    Found existing installation: langchain-community 0.3.27\n",
      "   ------------------------------------ --- 24/26 [langchain]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "    Uninstalling langchain-community-0.3.27:\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "      Successfully uninstalled langchain-community-0.3.27\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   -------------------------------------- - 25/26 [langchain-community]\n",
      "   ---------------------------------------- 26/26 [langchain-community]\n",
      "\n",
      "Successfully installed aiohttp-3.10.11 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 httpx-0.28.1 huggingface-hub-0.35.3 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.79 langchain-pinecone-0.2.5 langchain-tests-0.3.22 langsmith-0.3.45 markdown-it-py-4.0.0 mdurl-0.1.2 multiprocess-0.70.16 pinecone-6.0.2 pinecone-plugin-interface-0.0.7 py-cpuinfo-9.0.0 pytest-benchmark-5.1.0 pytest-codspeed-4.1.1 pytest-recording-0.13.4 pytest-socket-0.7.0 rich-14.2.0 syrupy-4.9.1 vcrpy-7.0.0 wrapt-1.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pinecone 6.0.2 does not provide the extra 'async'\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.3.23 langchain-community==0.3.21 langchain-pinecone==0.2.5 langchain_groq datasets==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00464f",
   "metadata": {},
   "source": [
    "## Load API Keys from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aa0b2",
   "metadata": {},
   "source": [
    "### What is env file?\n",
    "\n",
    "* A .env file is a simple text file that stores environment variables (like API keys and secrets) in key=value format.\n",
    "* Example content of a .env file:\n",
    "* PINECONE_API_KEY=your_pinecone_api_key_here\n",
    "* GROQ_API_KEY=your_groq_api_key_here\n",
    "\n",
    "* It helps keep sensitive information out of your code and makes it easier to manage secrets securely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a4c86",
   "metadata": {},
   "source": [
    "**Imports tools** to:\n",
    "\n",
    "* Use environment variables (`os`)\n",
    "* Load values from a `.env` file (`load_dotenv`)\n",
    "* **os** is a Python built-in module that lets your code interact with the operating system (like Windows, macOS, Linux).\n",
    "---\n",
    "* **Loads the `.env` file** so Python can use the secret keys stored in it (like API keys).\n",
    "* **Gets the values** of `PINECONE_API_KEY` and `GROQ_API_KEY` from the `.env` file.\n",
    "* **In Short:** This code **reads your secret keys from a `.env` file** so you don’t have to write them directly in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e6bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone API Key Loaded Successfully\n",
      "✅ Groq API Key Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the keys\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Check if keys are loaded properly\n",
    "if pinecone_key:\n",
    "    print(\"✅ Pinecone API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Pinecone API Key NOT Loaded\")\n",
    "\n",
    "if groq_key:\n",
    "    print(\"✅ Groq API Key Loaded Successfully\")\n",
    "else:\n",
    "    print(\"❌ Groq API Key NOT Loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1c075",
   "metadata": {},
   "source": [
    "## What is `langchain_groq`?\n",
    "\n",
    "`langchain_groq` is a **LangChain integration** that lets you **connect to Groq’s LLMs** (like LLaMA3) easily.\n",
    "\n",
    "Think of it as a **bridge between LangChain and Groq’s fast language models**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `ChatGroq`?\n",
    "\n",
    "`ChatGroq` is a **class (tool)** inside `langchain_groq`.\n",
    "\n",
    "It lets you:\n",
    "\n",
    "* **Send prompts** to Groq-hosted models\n",
    "* **Receive responses** from those models\n",
    "* Use these models in your **LangChain app**, like chatbots, RAG, agents, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we use this?**\n",
    "\n",
    "Instead of manually setting up HTTP requests to Groq’s API, `ChatGroq` makes it **super easy**:\n",
    "\n",
    "* Lets you talk to a specific Groq model (`llama3-8b-8192`)\n",
    "* Works smoothly with LangChain tools (retrievers, chains, memory, etc.)\n",
    "* Connects securely with your `groq_api_key`\n",
    "\n",
    "---\n",
    "\n",
    "#### In Simple Words\n",
    "\n",
    "* `langchain_groq` lets LangChain talk to Groq.\n",
    "* `ChatGroq` is the tool that helps you **chat with Groq’s AI model** using your API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990bfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(\n",
    "    groq_api_key=groq_key,\n",
    "    model_name=\"Llama-3.3-70B-Versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09520cec",
   "metadata": {},
   "source": [
    "Groq uses the **same chat structure as OpenAI** because it runs **OpenAI-compatible models** like `llama3`, `mixtral`, etc.\n",
    "So just like OpenAI, chats with Groq **typically look like this in plain text**:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "User: Hi, how are you?\n",
    "Assistant: I'm doing well! How can I assist you today?\n",
    "User: What is quantum computing?\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "---\n",
    "\n",
    "**In Code (OpenAI/Groq-compatible format):**\n",
    "\n",
    "When using the API (like with `ChatGroq` or `ChatOpenAI` in LangChain), you use this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**In LangChain (message objects):**\n",
    "\n",
    "LangChain wraps those into **message classes**, like:\n",
    "\n",
    "```\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is quantum computing?\")\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21b712",
   "metadata": {},
   "source": [
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "Then you pass them to the model:\n",
    "\n",
    "```\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8254e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab48e",
   "metadata": {},
   "source": [
    "We generate the next response from the AI by passing these messages to the `ChatGroq` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ddade",
   "metadata": {},
   "source": [
    "Like saying to the AI:\n",
    "\n",
    "“Here’s what has been said so far — now tell me what the AI should say next.”\n",
    "\n",
    "LangChain then handles formatting and sending this to the LLM backend, and res stores the AI’s next reply.\n",
    "\n",
    "**In Short:**\n",
    "* You define a conversation (via messages).\n",
    "* Call the LLM using chat(messages).\n",
    "* Get a response back — stored in res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109d2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas\\AppData\\Local\\Temp\\ipykernel_17860\\1671545441.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = chat(messages)\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d420165f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='String theory is a complex and abstract concept in physics, but I\\'ll try to break it down in simple terms.\\n\\n**What is string theory?**\\n\\nString theory is a theoretical framework in physics that attempts to reconcile two major areas of physics: general relativity (which describes gravity and the large-scale structure of the universe) and quantum mechanics (which describes the behavior of particles at the atomic and subatomic level).\\n\\nThe core idea of string theory is that the fundamental building blocks of the universe are not particles (like electrons and photons), but tiny, vibrating strings. These strings are thought to be the source of all particles and forces in the universe.\\n\\n**Key concepts:**\\n\\n1. **Strings**: The fundamental objects in string theory are one-dimensional strings, rather than point-like particles. These strings can vibrate at different frequencies, giving rise to various particles.\\n2. **Vibrational modes**: The vibrational modes of the strings correspond to different particles, such as electrons, photons, and quarks. Each mode represents a specific energy level and particle type.\\n3. **Dimensions**: String theory requires the existence of more than the three spatial dimensions (length, width, and height) and one time dimension that we experience in everyday life. The theory proposes that there are six additional dimensions, which are \"compactified\" or \"curled up\" so tightly that they\\'re not directly observable.\\n4. **Calabi-Yau manifolds**: These are complex geometric structures that describe the compactification of the extra dimensions. They play a crucial role in determining the properties of particles and forces in the universe.\\n5. **Superstrings**: String theory also involves the concept of superstrings, which are strings that exhibit supersymmetry (a symmetry between bosons and fermions, two types of particles).\\n\\n**Challenges and criticisms:**\\n\\n1. **Lack of experimental evidence**: Despite decades of research, there is currently no direct experimental evidence to support string theory.\\n2. **Mathematical complexity**: String theory is incredibly mathematically demanding, making it challenging to develop and test the theory.\\n3. **Multiverse problem**: String theory predicts the existence of a vast multiverse, which raises concerns about the theory\\'s predictive power and testability.\\n\\n**Current status:**\\n\\nString theory is an active area of research, with many physicists working to develop and refine the theory. While it has generated a lot of excitement and debate, it remains a speculative framework that requires further experimentation and validation to become a widely accepted theory.\\n\\n**Where to start:**\\n\\nIf you\\'re interested in learning more about string theory, I recommend starting with some introductory resources, such as:\\n\\n1. **Popular science books**: \"The Elegant Universe\" by Brian Greene, \"A Brief History of Time\" by Stephen Hawking, and \"Warped Passages\" by Lisa Randall.\\n2. **Online lectures**: Lectures by physicists like Leonard Susskind, Nima Arkani-Hamed, and Andrew Strominger on platforms like YouTube, edX, or Coursera.\\n3. **Research papers**: Look for papers on arXiv or other academic databases to delve deeper into the technical aspects of string theory.\\n\\nKeep in mind that string theory is a complex and abstract subject, and it may take time to develop a deeper understanding. I\\'m here to help you navigate the basics and provide guidance on where to find more information.\\n\\nWhat specific aspects of string theory would you like to explore further?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 698, 'prompt_tokens': 79, 'total_tokens': 777, 'completion_time': 1.5681184080000001, 'prompt_time': 0.003992197, 'queue_time': 0.049221433, 'total_time': 1.572110605}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--282a07f1-6366-4576-b720-03890c4018d1-0', usage_metadata={'input_tokens': 79, 'output_tokens': 698, 'total_tokens': 777})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f108837",
   "metadata": {},
   "source": [
    "To see the models reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748f8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a complex and abstract concept in physics, but I'll try to break it down in simple terms.\n",
      "\n",
      "**What is string theory?**\n",
      "\n",
      "String theory is a theoretical framework in physics that attempts to reconcile two major areas of physics: general relativity (which describes gravity and the large-scale structure of the universe) and quantum mechanics (which describes the behavior of particles at the atomic and subatomic level).\n",
      "\n",
      "The core idea of string theory is that the fundamental building blocks of the universe are not particles (like electrons and photons), but tiny, vibrating strings. These strings are thought to be the source of all particles and forces in the universe.\n",
      "\n",
      "**Key concepts:**\n",
      "\n",
      "1. **Strings**: The fundamental objects in string theory are one-dimensional strings, rather than point-like particles. These strings can vibrate at different frequencies, giving rise to various particles.\n",
      "2. **Vibrational modes**: The vibrational modes of the strings correspond to different particles, such as electrons, photons, and quarks. Each mode represents a specific energy level and particle type.\n",
      "3. **Dimensions**: String theory requires the existence of more than the three spatial dimensions (length, width, and height) and one time dimension that we experience in everyday life. The theory proposes that there are six additional dimensions, which are \"compactified\" or \"curled up\" so tightly that they're not directly observable.\n",
      "4. **Calabi-Yau manifolds**: These are complex geometric structures that describe the compactification of the extra dimensions. They play a crucial role in determining the properties of particles and forces in the universe.\n",
      "5. **Superstrings**: String theory also involves the concept of superstrings, which are strings that exhibit supersymmetry (a symmetry between bosons and fermions, two types of particles).\n",
      "\n",
      "**Challenges and criticisms:**\n",
      "\n",
      "1. **Lack of experimental evidence**: Despite decades of research, there is currently no direct experimental evidence to support string theory.\n",
      "2. **Mathematical complexity**: String theory is incredibly mathematically demanding, making it challenging to develop and test the theory.\n",
      "3. **Multiverse problem**: String theory predicts the existence of a vast multiverse, which raises concerns about the theory's predictive power and testability.\n",
      "\n",
      "**Current status:**\n",
      "\n",
      "String theory is an active area of research, with many physicists working to develop and refine the theory. While it has generated a lot of excitement and debate, it remains a speculative framework that requires further experimentation and validation to become a widely accepted theory.\n",
      "\n",
      "**Where to start:**\n",
      "\n",
      "If you're interested in learning more about string theory, I recommend starting with some introductory resources, such as:\n",
      "\n",
      "1. **Popular science books**: \"The Elegant Universe\" by Brian Greene, \"A Brief History of Time\" by Stephen Hawking, and \"Warped Passages\" by Lisa Randall.\n",
      "2. **Online lectures**: Lectures by physicists like Leonard Susskind, Nima Arkani-Hamed, and Andrew Strominger on platforms like YouTube, edX, or Coursera.\n",
      "3. **Research papers**: Look for papers on arXiv or other academic databases to delve deeper into the technical aspects of string theory.\n",
      "\n",
      "Keep in mind that string theory is a complex and abstract subject, and it may take time to develop a deeper understanding. I'm here to help you navigate the basics and provide guidance on where to find more information.\n",
      "\n",
      "What specific aspects of string theory would you like to explore further?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d2d69",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c75ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory, also known as a \"theory of everything\" (ToE), for several reasons:\n",
      "\n",
      "1. **Unification of forces**: String theory attempts to unify the fundamental forces of nature, including:\n",
      "\t* Gravity (described by general relativity)\n",
      "\t* Electromagnetism (described by quantum electrodynamics)\n",
      "\t* Strong nuclear force (described by quantum chromodynamics)\n",
      "\t* Weak nuclear force (described by the electroweak theory)\n",
      "\t* String theory posits that these forces are different vibrational modes of the same underlying string.\n",
      "2. **Consistency with quantum mechanics and general relativity**: String theory is an attempt to reconcile the principles of quantum mechanics (which describe the behavior of particles at the atomic and subatomic level) with the principles of general relativity (which describe gravity and the large-scale structure of the universe). By positing that particles are vibrations of strings, string theory aims to provide a consistent description of both the small-scale (quantum) and large-scale (gravitational) phenomena.\n",
      "3. **Extra dimensions**: The additional dimensions in string theory (beyond the three spatial dimensions and one time dimension we experience) provide a framework for unifying the forces. These extra dimensions are \"compactified\" or \"curled up\" in a way that gives rise to the various forces and particles we observe.\n",
      "4. **Renormalization**: String theory is a renormalizable theory, meaning that it can be formulated in a way that eliminates infinite quantities and provides a consistent, finite description of physical phenomena. This is a crucial property for a unified theory, as it allows for precise predictions and calculations.\n",
      "5. **Mathematical structure**: String theory is built upon a rich mathematical structure, which includes:\n",
      "\t* Calabi-Yau manifolds (complex geometric structures that describe the compactification of extra dimensions)\n",
      "\t* Differential geometry (which describes the curvature and topology of spacetime)\n",
      "\t* Topology (which studies the properties of shapes and spaces)\n",
      "\t* This mathematical framework provides a powerful tool for describing the behavior of particles and forces in a unified way.\n",
      "6. **Predictive power**: String theory makes predictions about the behavior of particles and forces at very high energies, which could be tested in future experiments. If these predictions are confirmed, it would provide strong evidence for the validity of string theory as a unified theory.\n",
      "7. **Solving the hierarchy problem**: String theory offers a potential solution to the hierarchy problem, which is the question of why the gravitational force is so much weaker than the other fundamental forces. String theory provides a mechanism for explaining this hierarchy, which is a crucial aspect of a unified theory.\n",
      "\n",
      "While string theory is still a speculative framework, these features have led many physicists to believe that it has the potential to provide a unified theory of everything. However, much work remains to be done to develop and test the theory, and it is still unclear whether string theory will ultimately succeed in providing a complete and consistent description of the universe.\n",
      "\n",
      "**Challenges and open questions:**\n",
      "\n",
      "1. **Lack of experimental evidence**: Despite decades of research, there is currently no direct experimental evidence to support string theory.\n",
      "2. **Mathematical complexity**: String theory is incredibly mathematically demanding, making it challenging to develop and test the theory.\n",
      "3. **Multiverse problem**: String theory predicts the existence of a vast multiverse, which raises concerns about the theory's predictive power and testability.\n",
      "4. **Lack of a complete formulation**: String theory is still an incomplete theory, and a fully consistent and complete formulation is still lacking.\n",
      "\n",
      "These challenges highlight the need for continued research and experimentation to determine whether string theory can indeed provide a unified theory of everything.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c7774",
   "metadata": {},
   "source": [
    "## Dealing with Hallucinations\n",
    "\n",
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about Deepseek R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2b6ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory, also known as a \"theory of everything\" (ToE), for several reasons:\n",
      "\n",
      "1. **Unification of forces**: String theory attempts to unify the fundamental forces of nature, including:\n",
      "\t* Gravity (described by general relativity)\n",
      "\t* Electromagnetism (described by quantum electrodynamics)\n",
      "\t* Strong nuclear force (described by quantum chromodynamics)\n",
      "\t* Weak nuclear force (described by the electroweak theory)\n",
      "\t* String theory posits that these forces are different vibrational modes of the same underlying string.\n",
      "2. **Consistency with quantum mechanics and general relativity**: String theory is an attempt to reconcile the principles of quantum mechanics (which describe the behavior of particles at the atomic and subatomic level) with the principles of general relativity (which describe gravity and the large-scale structure of the universe). By positing that particles are vibrations of strings, string theory aims to provide a consistent description of both the small-scale (quantum) and large-scale (gravitational) phenomena.\n",
      "3. **Extra dimensions**: The additional dimensions in string theory (beyond the three spatial dimensions and one time dimension we experience) provide a framework for unifying the forces. These extra dimensions are \"compactified\" or \"curled up\" in a way that gives rise to the various forces and particles we observe.\n",
      "4. **Renormalization**: String theory is a renormalizable theory, meaning that it can be formulated in a way that eliminates infinite quantities and provides a consistent, finite description of physical phenomena. This is a crucial property for a unified theory, as it allows for precise predictions and calculations.\n",
      "5. **Mathematical structure**: String theory is built upon a rich mathematical structure, which includes:\n",
      "\t* Calabi-Yau manifolds (complex geometric structures that describe the compactification of extra dimensions)\n",
      "\t* Differential geometry (which describes the curvature and topology of spacetime)\n",
      "\t* Topology (which studies the properties of shapes and spaces)\n",
      "\t* This mathematical framework provides a powerful tool for describing the behavior of particles and forces in a unified way.\n",
      "6. **Predictive power**: String theory makes predictions about the behavior of particles and forces at very high energies, which could be tested in future experiments. If these predictions are confirmed, it would provide strong evidence for the validity of string theory as a unified theory.\n",
      "7. **Solving the hierarchy problem**: String theory offers a potential solution to the hierarchy problem, which is the question of why the gravitational force is so much weaker than the other fundamental forces. String theory provides a mechanism for explaining this hierarchy, which is a crucial aspect of a unified theory.\n",
      "\n",
      "While string theory is still a speculative framework, these features have led many physicists to believe that it has the potential to provide a unified theory of everything. However, much work remains to be done to develop and test the theory, and it is still unclear whether string theory will ultimately succeed in providing a complete and consistent description of the universe.\n",
      "\n",
      "**Challenges and open questions:**\n",
      "\n",
      "1. **Lack of experimental evidence**: Despite decades of research, there is currently no direct experimental evidence to support string theory.\n",
      "2. **Mathematical complexity**: String theory is incredibly mathematically demanding, making it challenging to develop and test the theory.\n",
      "3. **Multiverse problem**: String theory predicts the existence of a vast multiverse, which raises concerns about the theory's predictive power and testability.\n",
      "4. **Lack of a complete formulation**: String theory is still an incomplete theory, and a fully consistent and complete formulation is still lacking.\n",
      "\n",
      "These challenges highlight the need for continued research and experimentation to determine whether string theory can indeed provide a unified theory of everything.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8794be3e",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e10521",
   "metadata": {},
   "source": [
    "## Alternate Way : Source Knowledge\n",
    "\n",
    "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the Deepseek question. We can take the paper abstract from the [Deepseek R1 paper](https://arxiv.org/abs/2501.12948)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af49e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and \"\n",
    "    \"DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale \"\n",
    "    \"reinforcement learning (RL) without supervised fine-tuning (SFT) as a \"\n",
    "    \"preliminary step, demonstrates remarkable reasoning capabilities. Through \"\n",
    "    \"RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and \"\n",
    "    \"intriguing reasoning behaviors. However, it encounters challenges such as \"\n",
    "    \"poor readability, and language mixing. To address these issues and \"\n",
    "    \"further enhance reasoning performance, we introduce DeepSeek-R1, which \"\n",
    "    \"incorporates multi-stage training and cold-start data before RL. \"\n",
    "    \"DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on \"\n",
    "    \"reasoning tasks. To support the research community, we open-source \"\n",
    "    \"DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, \"\n",
    "    \"32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffaa08",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40297f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf6bb0",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22858032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb86eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, DeepSeek-R1 is special because it achieves performance comparable to OpenAI-o1-1217 on reasoning tasks, thanks to its multi-stage training and incorporation of cold-start data before reinforcement learning (RL). This suggests that DeepSeek-R1 has made significant advancements in reasoning capabilities, making it a notable model in the field. Additionally, the fact that its predecessor, DeepSeek-R1-Zero, was able to demonstrate remarkable reasoning capabilities without supervised fine-tuning (SFT) is also a notable achievement, and DeepSeek-R1 builds upon this foundation to further enhance its performance.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b235b09",
   "metadata": {},
   "source": [
    "## How do we get this information in the first place?\n",
    "\n",
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b79b1",
   "metadata": {},
   "source": [
    "This is where Pinecone and vector databases comes in place, as they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b633b59",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/deepseek-r1-paper-chunked\"` dataset. This dataset contains the Deepseek R1 paper pre-processed into RAG-ready chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a0a1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Anas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anas\\.cache\\huggingface\\hub\\datasets--jamescalam--deepseek-r1-paper-chunked. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|█████████████████████████████████████████████████| 76/76 [00:00<00:00, 2238.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'num_tokens', 'pages', 'source'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/deepseek-r1-paper-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c9178a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2501.12948v1',\n",
       " 'chunk-id': 1,\n",
       " 'chunk': \"uestion: If a > 1, then the sum of the real solutions of √a - √a + x = x is equal to Response: <think> To solve the equation √a – √a + x = x, let's start by squaring both . . . (√a-√a+x)² = x² ⇒ a - √a + x = x². Rearrange to isolate the inner square root term:(a – x²)² = a + x ⇒ a² – 2ax² + (x²)² = a + x ⇒ x⁴ - 2ax² - x + (a² – a) = 0\",\n",
       " 'num_tokens': 145,\n",
       " 'pages': [1],\n",
       " 'source': 'https://arxiv.org/abs/2501.12948'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81d343",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Deepseek R1 ArXiv papers. Each entry in the dataset represents a \"chunk\" of text from the R1 paper.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, even many of the newest LLMs cannot answer questions about Deepseek R1 — at least not without this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a0ede",
   "metadata": {},
   "source": [
    "## Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our Pinecone client, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3e9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize client\n",
    "pc = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78b484",
   "metadata": {},
   "source": [
    "Delete the old one to save the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aa6f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag1\"\n",
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "545e1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec, CloudProvider, AwsRegion, Metric\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    metric=Metric.DOTPRODUCT,\n",
    "    dimension=384,  # ✅ match your embedding model\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1\n",
    "    )\n",
    ")\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df29057",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will HuggingFace's `sentence-transformers/all-MiniLM-L6-v2` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59cf9844-ba96-4ab9-a15d-c2d181219aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.29)\n",
      "Requirement already satisfied: requests in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\anas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 8.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/12.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 13.6 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 24.3 MB/s  0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 3.1/8.7 MB 15.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.7/8.7 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.2/8.7 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.6/8.7 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.7 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 6.6 MB/s  0:00:01\n",
      "Downloading scipy-1.16.2-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.5 MB 5.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.1/38.5 MB 5.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.4/38.5 MB 5.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.5/38.5 MB 5.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 5.5/38.5 MB 5.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.6/38.5 MB 5.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 7.9/38.5 MB 5.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 8.9/38.5 MB 5.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.0/38.5 MB 5.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 11.3/38.5 MB 5.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 12.3/38.5 MB 5.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 13.6/38.5 MB 5.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 14.9/38.5 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/38.5 MB 5.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 17.3/38.5 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.6/38.5 MB 5.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.7/38.5 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.0/38.5 MB 5.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 22.5/38.5 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.1/38.5 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.7/38.5 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.3/38.5 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.8/38.5 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.4/38.5 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.2/38.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 33.8/38.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.7/38.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.7/38.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 6.4 MB/s  0:00:06\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, safetensors, scikit-learn, tokenizers, transformers, sentence-transformers\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----- ---------------------------------- 1/7 [scipy]\n",
      "   ----------- ---------------------------- 2/7 [safetensors]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ----------------- ---------------------- 3/7 [scikit-learn]\n",
      "   ---------------------- ----------------- 4/7 [tokenizers]\n",
      "   ---------------------- ----------------- 4/7 [tokenizers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------- ----- 6/7 [sentence-transformers]\n",
      "   ---------------------------------------- 7/7 [sentence-transformers]\n",
      "\n",
      "Successfully installed safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-5.1.1 threadpoolctl-3.6.0 tokenizers-0.22.1 transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd405ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9229b3d",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5daf97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ea6f5",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) CHANGE-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88b28f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e4fa5",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f56bb113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e988a",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86c59e",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to link that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb42eae",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b9846ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embed_model,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799256a3",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3214e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2501.12948v1-39', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.'),\n",
       " Document(id='2501.12948v1-56', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nTable 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \\nNotably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \\nOverall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\n| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\\n| --- | --- | --- | --- | --- | --- |\\n|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\\n| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\\n| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\\n| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\\n| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\\n| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\\n| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\\n| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\\n| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\\n| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |'),\n",
       " Document(id='2501.12948v1-4', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nThe table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \\nDeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \\nAt lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\\n|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\\n| --- | --- | --- | --- | --- | --- |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n| 100 |  |  |  |  |  |\\n|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\\n|  |  | 93.4 |  | 94.3 |  |\\n|  |  | 90.6 |  | 90.0 90.2 |  |\\n| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\\n|  |  |  |  |  | 87.4 |\\n|  |  |  |  |  | 85.2 |\\n|  | 72.6 |  |  |  |  |\\n|  |  |  | 75.7 |  |  |\\n|  |  |  | 71.5 |  |  |\\n|  | 63.6 |  | 62.1 |  |  |\\n|  |  |  | 60.0 59.1 |  |  |\\n| 60 |  | 58.7 |  |  |  |\\n| Accuracy / Percentile (%) |  |  |  |  |  |\\n| 40 |  |  |  |  |  |\\n|  | 39.2 |  |  |  |  |\\n| 20 |  |  |  |  |  |\\n| 0 |  |  |  |  |  |\\n|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb169",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to link the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98c418",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25a15337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a526be0",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "769766aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    ## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.\n",
      "Table summary:\n",
      "Table 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \n",
      "Notably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \n",
      "Overall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\n",
      "Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\n",
      "reasoning-related benchmarks.\n",
      "| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\n",
      "| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\n",
      "| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\n",
      "| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\n",
      "| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\n",
      "| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\n",
      "| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\n",
      "| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |\n",
      "Table summary:\n",
      "The table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \n",
      "DeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \n",
      "At lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\n",
      "|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "| 100 |  |  |  |  |  |\n",
      "|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\n",
      "|  |  | 93.4 |  | 94.3 |  |\n",
      "|  |  | 90.6 |  | 90.0 90.2 |  |\n",
      "| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\n",
      "|  |  |  |  |  | 87.4 |\n",
      "|  |  |  |  |  | 85.2 |\n",
      "|  | 72.6 |  |  |  |  |\n",
      "|  |  |  | 75.7 |  |  |\n",
      "|  |  |  | 71.5 |  |  |\n",
      "|  | 63.6 |  | 62.1 |  |  |\n",
      "|  |  |  | 60.0 59.1 |  |  |\n",
      "| 60 |  | 58.7 |  |  |  |\n",
      "| Accuracy / Percentile (%) |  |  |  |  |  |\n",
      "| 40 |  |  |  |  |  |\n",
      "|  | 39.2 |  |  |  |  |\n",
      "| 20 |  |  |  |  |  |\n",
      "| 0 |  |  |  |  |  |\n",
      "|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "\n",
      "    Query: What is so special about Deepseek R1?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e5e8",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8a399de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1 is special because it achieves competitive performance in various reasoning tasks, often surpassing or matching the performance of other leading models like OpenAI-01-1217. Some notable aspects of DeepSeek-R1 include:\n",
      "\n",
      "1. **High scores on reasoning benchmarks**: DeepSeek-R1 achieves impressive scores on benchmarks like AIME 2024 (79.8% Pass@1), MATH-500 (97.3%), and GPQA Diamond (71.5%).\n",
      "2. **Expert-level performance on coding tasks**: DeepSeek-R1 demonstrates expert-level performance on coding-related tasks, with an Elo rating of 2029 on Codeforces, outperforming 96.3% of human participants.\n",
      "3. **Competitive edge in educational tasks**: DeepSeek-R1 performs well on knowledge-related benchmarks like MMLU, MMLU-Pro, and GPQA Diamond, making it a strong contender for educational applications.\n",
      "4. **Robust performance across distilled models**: The distilled models of DeepSeek-R1, such as DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B, consistently rank high across multiple benchmarks, demonstrating the effectiveness of the DeepSeek-R1 architecture.\n",
      "5. **Strong performance at high accuracy levels**: DeepSeek-R1 excels in higher accuracy ranges, with scores of 96.3% and 96.6% at the highest accuracy level (100%), indicating its potential for applications requiring high precision.\n",
      "\n",
      "Overall, DeepSeek-R1's impressive performance across various reasoning tasks and benchmarks makes it a notable model in the field of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d917fe",
   "metadata": {},
   "source": [
    "We can continue with another Deepseek R1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cadefe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contexts provided do not offer a direct comparison between DeepSeek-R1 and DeepSeek-R1-Zero. However, it is mentioned that DeepSeek-R1 was introduced to address the challenges faced by DeepSeek-R1-Zero, such as poor readability and language mixing.\n",
      "\n",
      "DeepSeek-R1-Zero is described as a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, which demonstrates remarkable reasoning capabilities. On the other hand, DeepSeek-R1 incorporates multi-stage training and cold-start data before RL, which helps to enhance its reasoning performance.\n",
      "\n",
      "While the exact comparison between DeepSeek-R1 and DeepSeek-R1-Zero is not provided, it can be inferred that DeepSeek-R1 is an improved version of DeepSeek-R1-Zero, with better performance and addressing some of the challenges faced by its predecessor. However, the specific details of the comparison are not available in the given contexts.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"how does deepseek r1 compare to deepseek r1 zero?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef84ba",
   "metadata": {},
   "source": [
    "You can continue asking questions about Deepseek R1, but once you're done you can delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49ac1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
